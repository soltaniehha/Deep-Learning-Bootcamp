{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"jdjaqgyso-U6"},"outputs":[],"source":["from IPython.display import Pretty as disp\n","hint = 'https://raw.githubusercontent.com/soltaniehha/Business-Analytics/master/docs/hints/'  # path to hints on GitHub\n","\n","import pandas as pd\n","import seaborn as sns"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"wBuqPd7Xo-U7"},"source":["### Classification: Predicting discrete labels\n","\n","We will first take a look at a simple *classification* task, in which you are given a set of labeled points and want to use these to classify some unlabeled points.\n","\n","Imagine that we have the data shown in this figure:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"V7D4JSpLo-U8"},"source":["\n","<img src=\"https://github.com/soltaniehha/Business-Analytics/blob/master/figs/11-02-classification-1.png?raw=true\" width=\"600\" align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"AynsdHR9o-U8"},"source":["Here we have two-dimensional data: that is, we have two *features* for each point, represented by the *(x,y)* positions of the points on the plane.\n","In addition, we have one of two *class labels* for each point, here represented by the colors of the points.\n","From these features and labels, we would like to create a model that will let us decide whether a new point should be labeled \"blue\" or \"red.\"\n","\n","There are a number of possible models for such a classification task, but here we will use an extremely simple one. We will make the assumption that the two groups can be separated by drawing a straight line through the plane between them, such that points on each side of the line fall in the same group.\n","Here the *model* is a quantitative version of the statement \"a straight line separates the classes\", while the *model parameters* are the particular numbers describing the location and orientation of that line for our data.\n","The optimal values for these model parameters are learned from the data (this is the \"learning\" in machine learning), which is often called *training the model*.\n","\n","The following figure shows a visual representation of what the trained model looks like for this data:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"XAp24eTwo-U8"},"source":["\n","<img src=\"https://github.com/soltaniehha/Business-Analytics/blob/master/figs/11-02-classification-2.png?raw=true\" width=\"600\" align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"ozDGvlomo-U8"},"source":["Now that this model has been trained, it can be generalized to new, unlabeled data.\n","In other words, we can take a new set of data, draw this model line through it, and assign labels to the new points based on this model.\n","This stage is usually called *prediction*. See the following figure:"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"9gujiRMto-U8"},"source":["\n","<img src=\"https://github.com/soltaniehha/Business-Analytics/blob/master/figs/11-02-classification-3.png?raw=true\" width=\"900\" align=\"center\"/>"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"iNto_k-Zo-U8"},"source":["This is the basic idea of a classification task in machine learning, where \"classification\" indicates that the data has discrete class labels.\n","At first glance this may look fairly trivial: it would be relatively easy to simply look at this data and draw such a discriminatory line to accomplish this classification.\n","A benefit of the machine learning approach, however, is that it can generalize to much larger datasets in many more dimensions.\n","\n","For example, this is similar to the task of automated spam detection for email; in this case, we might use the following features and labels:\n","\n","- *feature 1*, *feature 2*, etc. $\\to$ normalized counts of important words or phrases (\"Serious cash\", \"Nigerian prince\", etc.)\n","- *label* $\\to$ \"spam\" or \"not spam\"\n","\n","For the training set, these labels might be determined by individual inspection of a small representative sample of emails; for the remaining emails, the label would be determined using the model.\n","For a suitably trained classification algorithm with enough well-constructed features (typically thousands or millions of words or phrases), this type of approach can be very effective."]},{"cell_type":"markdown","metadata":{"id":"yg5BtGx3o-U8"},"source":["**Examples:**\n","\n","Here are a few of the multitude of ways classification can be used in the real world.\n","\n","**Predicting credit risk**\n","\n","A financing company might look at a number of variables before offering a loan to a company or individual. Whether or not to offer the loan is a binary classification problem.\n","\n","**News classification**\n","\n","An algorithm might be trained to predict the topic of a news article (sports, politics, business, etc.).\n","\n","**Classifying human activity**\n","\n","By collecting data from sensors such as a phone accelerometer or smart watch, you can predict the person’s activity. The output will be one of a finite set of classes (e.g., walking, sleeping, standing, or running).\n","\n","## Types of Classification\n","\n","Before we continue, let’s review several different types of classification.\n","\n","### Binary Classification\n","\n","The simplest example of classification is binary classification, where there are only two labels you can predict. One example is fraud analytics, where a given transaction can be classified as fraudulent or not; or email spam, where a given email can be classified as spam or not spam.\n","\n","### Multiclass Classification\n","\n","Beyond binary classification lies multiclass classification, where one label is chosen from more than two distinct possible labels. A typical example is Facebook predicting the people in a given photo or a meterologist predicting the weather (rainy, sunny, cloudy, etc.). Note how there is always a finite set of classes to predict; it’s never unbounded. This is also called multinomial classification.\n","\n","### Multilabel Classification\n","\n","Finally, there is multilabel classification, where a given input can produce multiple labels. For example, you might want to predict a book’s genre based on the text of the book itself. While this could be multiclass, it’s probably better suited for multilabel because a book may fall into multiple genres. Another example of multilabel classification is identifying the number of objects that appear in an image. Note that in this example, the number of output predictions is not necessarily fixed, and could vary from image to image."]},{"cell_type":"markdown","metadata":{"id":"ljikLQ-Qo-U8"},"source":["## Popular Classification Algorithms\n","\n","\n","\n","*   Logistic Regression - 1930s\n","*   Decision Trees - 1960s (ID3 in 1980s)\n","*   Random Forests - 2001\n","*   Gradient-boosted Trees - Late 1990s\n","*   Multilayer Perceptron Classifier - 1950s (Backpropagation in 1980s)"]},{"cell_type":"markdown","source":["From the early days of logistic regression in the 1930s, we've come a long way to the complex world of deep learning today. This growth in machine learning, starting from basic algorithms, shows how much the field has evolved. Modern AI systems, like deep networks, can trace their roots back to these simple beginnings, reminding us of the value of learning the basics in this rapidly changing field.\n","\n","Today, let's begin our journey with Logistic Regression.\n","\n","\n","\n"],"metadata":{"id":"RHaVVaqn8w9H"}},{"cell_type":"markdown","metadata":{"id":"JbP7UG9Ko-U8"},"source":["## Logistic Regression\n","\n","Logistic regression is one of the most popular methods of classification. It is a linear method that combines each of the individual inputs (or features) with specific weights (these weights are generated during the training process) that are then combined to get a probability of belonging to a particular class. These weights are helpful because they are good representations of feature importance; if you have a large weight, you can assume that variations in that feature have a significant effect on the outcome (assuming you performed normalization). A smaller weight means the feature is less likely to be important.\n","\n","Consider the following example: The *Default* Dataset"]},{"cell_type":"markdown","metadata":{"id":"kV-b0744o-U8"},"source":["<img src=\"https://github.com/soltaniehha/Business-Analytics/blob/master/figs/11-02-the-default-dataset.png?raw=true\" width=\"800\" align=\"center\"/>\n","\n","<div style=\"text-align: center\"> The Default data set. Left: The annual incomes and monthly credit card balances of a number of individuals. The individuals who defaulted on their credit card payments are shown in orange, and those who did not are shown in blue. Center: Boxplots of balance as a function of default status. Right: Boxplots of income as a function of default status. </div>\n","\n","*<div style=\"text-align: right\"> From [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) by Gareth James, et al </div>*"]},{"cell_type":"markdown","metadata":{"id":"SFG6gfgmo-U8"},"source":["Considering this data where the response \"default\" falls into a \"Yes\" or \"No\" category, let's find a model that can predict the probability of default by using the \"Balance\" on the credit card. Rather than modeling this response Y\n","directly, logistic regression models the probability that Y belongs to a particular category.\n","\n","The probability of default given balance can be written as:\n","> **Pr(**default = Yes|balance**)**\n","\n","The values of Pr(default = Yes|balance), which we abbreviate p(balance), will range between 0 and 1. Then for any given value of balance, a prediction can be made for default. For example, one might predict default = Yes for any individual for whom p(balance) > 0.5. Alternatively, if a company wishes to be conservative in predicting individuals who are at risk for default, then they may choose to use a lower threshold, such as p(balance) > 0.1."]},{"cell_type":"markdown","metadata":{"id":"WaYTpkTmo-U8"},"source":["\n","<img src=\"https://github.com/soltaniehha/Business-Analytics/blob/master/figs/11-02-Classification-lr.png?raw=true\" width=\"800\" align=\"center\"/>\n","\n","<div style=\"text-align: center\"> Classification using the Default data. Left: Estimated probability of default using linear regression. Some estimated probabilities are negative! The orange ticks indicate the 0/1 values coded for default(No or Yes). Right: Predicted probabilities of default using logistic regression. All probabilities lie between 0 and 1.\n"," </div>\n","\n","*<div style=\"text-align: right\"> From [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/) by Gareth James, et al </div>*"]},{"cell_type":"markdown","metadata":{"id":"zSdIvP3mo-U8"},"source":["### The Logistic Model\n","\n","How should we model the relationship between $p(X) = Pr(Y = 1|X)$ and $X$? (For convenience we are using the generic 0/1 coding for the response). One approach would be using a linear regression model to represent these probabilities:\n","\n","> $p(X) = \\beta_0 + \\beta_1 X$\n","\n","If we use this approach to predict default=Yes using balance, then we obtain the model shown in the left-hand panel of figure above. Here we see the problem with this approach: for balances close to zero we predict a negative probability of default; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of default, regardless of credit card balance, must fall between 0 and 1.\n","\n","To avoid this problem, we must model $p(X)$ using a function that gives outputs between 0 and 1 for all values of $X$. Many functions meet this description. In logistic regression, we use the logistic function:\n","\n","> $p(X) = \\dfrac{e ^ {\\beta_0 + \\beta_1 X }}{1 + e ^ {\\beta_0 + \\beta_1 X }}$"]},{"cell_type":"markdown","metadata":{"id":"2IvtgqB0o-U9"},"source":["To fit the model above, we use a method called `maximum likelihood`. The right-hand panel of figure above illustrates the fit of the logistic regression model to the Default data. Notice that for low balances we now predict the probability of default as close to, but never below, zero. Likewise, for high balances we predict a default probability close to, but never above, one. The logistic function will always produce an S-shaped curve of this form, and so regardless of the value of X, we will obtain a sensible prediction. We also see that the logistic model is\n","better able to capture the range of probabilities than is the linear regression model in the left-hand plot.\n","\n","After a bit of manipulation of the logistic function, we find that:\n","\n","> $\\dfrac{p(X)}{1-p(X)} = e ^ {\\beta_0 + \\beta_1 X }$\n","\n","The quantity $p(X)/[1−p(X)]$ is called the odds, and can take on any value between $0$ and $\\infty$. Values of the odds close to $0$ and $\\infty$ indicate very low and very high probabilities of default, respectively.\n","\n","For example, Let’s say that the probability of default is $0.2$ (or $1$ in $5$ people), this will give an odds of default equal to $1$ to $4$. This means that the odds of someone defaulting is $25\\%$ of not defaulting. Since $p(X) = 0.2$ implies an odds of $\\dfrac{0.2}{1-0.2} = 1/4$.\n","\n","Likewise on average $9$ out of every $10$ people with an odds of $9$ will default, since $p(X) = 0.9$ implies an odds of $\\dfrac{0.9}{1-0.9} = 9$.\n","\n","Odds are traditionally used instead of probabilities in horse-racing, since they relate more naturally to the correct betting strategy.\n","\n","By taking the logarithm of both sides of the equation above, we arrive at\n","\n","> $log(\\dfrac{p(X)}{1-p(X)}) = \\beta_0 + \\beta_1 X$\n"]},{"cell_type":"markdown","metadata":{"id":"EjihfYcDo-U9"},"source":["The left-hand side is called the *log-odds* or *logit*. We see that the logistic regression mdoel (defined above) has a logit that is linear in $X$.\n","\n","The table below shows estimated coefficients of the logistic regression model that predicts the probability of \"default\" using \"balance\":\n","\n","| - |Coefficient| Std. error| Z-statistic| P-value|\n","|--|--|--|--|--|\n","|Intercept| −10.6513| 0.3612| −29.5| <0.0001|\n","|balance |0.0055 |0.0002 |24.9 |<0.0001|"]},{"cell_type":"markdown","metadata":{"id":"r0iD1EYRo-U9"},"source":["We can measure the accuracy of the coefficient estimates by computing their standard errors. The z-statistic in table above plays the same role as the t-statistic in the linear regression output. A large (absolute) value of the z-statistic indicates evidence against the null hypothesis $H0 : β1 = 0$. This null hypothesis implies that $p(X) = \\dfrac{e ^ {\\beta_0 }}{1 + e ^ {\\beta_0 }}$. In other words, that the probability of default does not depend on balance. Since the p-value associated with balance in the table is tiny, we can reject $H0$. In other words, we conclude that there is indeed an association between balance and probability of default.\n","\n","The estimated intercept in the table is typically not of interest; its main purpose is to adjust the average fitted\n","probabilities to the proportion of ones in the data."]},{"cell_type":"markdown","metadata":{"id":"StqROPiHo-U9"},"source":["# Logistic Regression Example - Titanic survival dataset\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6QT5s2TOo-U9","outputId":"ac35aa51-a4e3-4c9e-a6cf-16f36fd4ea2d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>survived</th>\n","      <th>pclass</th>\n","      <th>sex</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked</th>\n","      <th>class</th>\n","      <th>who</th>\n","      <th>adult_male</th>\n","      <th>deck</th>\n","      <th>embark_town</th>\n","      <th>alive</th>\n","      <th>alone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>male</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>S</td>\n","      <td>Third</td>\n","      <td>man</td>\n","      <td>True</td>\n","      <td>NaN</td>\n","      <td>Southampton</td>\n","      <td>no</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>female</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>C</td>\n","      <td>First</td>\n","      <td>woman</td>\n","      <td>False</td>\n","      <td>C</td>\n","      <td>Cherbourg</td>\n","      <td>yes</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>female</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>S</td>\n","      <td>Third</td>\n","      <td>woman</td>\n","      <td>False</td>\n","      <td>NaN</td>\n","      <td>Southampton</td>\n","      <td>yes</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>female</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53.1000</td>\n","      <td>S</td>\n","      <td>First</td>\n","      <td>woman</td>\n","      <td>False</td>\n","      <td>C</td>\n","      <td>Southampton</td>\n","      <td>yes</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>male</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8.0500</td>\n","      <td>S</td>\n","      <td>Third</td>\n","      <td>man</td>\n","      <td>True</td>\n","      <td>NaN</td>\n","      <td>Southampton</td>\n","      <td>no</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n","0         0       3    male  22.0      1      0   7.2500        S  Third   \n","1         1       1  female  38.0      1      0  71.2833        C  First   \n","2         1       3  female  26.0      0      0   7.9250        S  Third   \n","3         1       1  female  35.0      1      0  53.1000        S  First   \n","4         0       3    male  35.0      0      0   8.0500        S  Third   \n","\n","     who  adult_male deck  embark_town alive  alone  \n","0    man        True  NaN  Southampton    no  False  \n","1  woman       False    C    Cherbourg   yes  False  \n","2  woman       False  NaN  Southampton   yes   True  \n","3  woman       False    C  Southampton   yes  False  \n","4    man        True  NaN  Southampton    no   True  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["titanic = sns.load_dataset('titanic')\n","titanic.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBSG4gIpo-U9","outputId":"673ca9f6-9cac-4f84-a6dd-bce3287b6901"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 891 entries, 0 to 890\n","Data columns (total 15 columns):\n","survived       891 non-null int64\n","pclass         891 non-null int64\n","sex            891 non-null object\n","age            714 non-null float64\n","sibsp          891 non-null int64\n","parch          891 non-null int64\n","fare           891 non-null float64\n","embarked       889 non-null object\n","class          891 non-null category\n","who            891 non-null object\n","adult_male     891 non-null bool\n","deck           203 non-null category\n","embark_town    889 non-null object\n","alive          891 non-null object\n","alone          891 non-null bool\n","dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n","memory usage: 80.6+ KB\n"]}],"source":["titanic.info()"]},{"cell_type":"markdown","metadata":{"id":"DbKfjQZNo-U9"},"source":["## Preprocessing - missing values:\n","\n","1. 177 missing values for \"age\"; we will replace them with 28 which is the median age.\n","2. 2 missing values in \"embarked\"; we will impute with \"S\" (the most common boarding port).\n","3. Too many missing values in \"deck\". We won't use this field and will drop it.\n","4. \"embarked\" is the short version of \"embark_town\"; we will drop \"embark_town\"\n","5. \"survived\" is our target variable `y`. Note that we also have another variable \"alive\" which is equivalent to \"survived\". We will drop that too.\n","6. \"pclass\" is the numeric representation of \"class\"; we will drop \"class\"\n","7. Information in \"sex\" is captured in \"who\"; we will drop \"sex\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LlOmkkgCo-U9","outputId":"3137735d-6eb8-41fb-ba61-77c750273229"},"outputs":[{"data":{"text/plain":["survived      0\n","pclass        0\n","age           0\n","sibsp         0\n","parch         0\n","fare          0\n","embarked      0\n","who           0\n","adult_male    0\n","alone         0\n","dtype: int64"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["titanic[\"age\"].fillna(titanic[\"age\"].median(skipna=True), inplace=True)                # replace 177 missing age by the median\n","titanic[\"embarked\"].fillna(titanic['embarked'].value_counts().idxmax(), inplace=True)  # replace 2 missing embarked by most common\n","titanic.drop(['deck','embark_town', 'alive', 'class', 'sex'], axis=1, inplace=True)    # drop deck, embark_town, alive, class & sex\n","titanic.isnull().sum()  # check for missing value"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Em7KYeQco-U9","outputId":"020067fe-dfe1-4b7b-cc90-6671fab77582"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>survived</th>\n","      <th>pclass</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked</th>\n","      <th>who</th>\n","      <th>adult_male</th>\n","      <th>alone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>S</td>\n","      <td>man</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>C</td>\n","      <td>woman</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>S</td>\n","      <td>woman</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53.1000</td>\n","      <td>S</td>\n","      <td>woman</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8.0500</td>\n","      <td>S</td>\n","      <td>man</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   survived  pclass   age  sibsp  parch     fare embarked    who  adult_male  \\\n","0         0       3  22.0      1      0   7.2500        S    man        True   \n","1         1       1  38.0      1      0  71.2833        C  woman       False   \n","2         1       3  26.0      0      0   7.9250        S  woman       False   \n","3         1       1  35.0      1      0  53.1000        S  woman       False   \n","4         0       3  35.0      0      0   8.0500        S    man        True   \n","\n","   alone  \n","0  False  \n","1  False  \n","2   True  \n","3  False  \n","4   True  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["titanic.head()"]},{"cell_type":"markdown","metadata":{"id":"Vv4DyEAHo-U-"},"source":["## Preprocessing - categorical variables:\n","\n","We will make sure all of our variables are numerical by converting the categorical to *dummy* variables. We can do this by pandas `get_dummies()` function. `get_dummies()` convert categorical variable into dummy/indicator variables, meaning that if we have 3 categories it will create 3 variables with modified column names to indicate the category. We will always need n-1 dummy variables when having n categories, so we will drop the extra one."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IvY8IW4Wo-U-","outputId":"3c086240-4490-4d26-90f1-8c909e35a68d"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>survived</th>\n","      <th>pclass</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked_Q</th>\n","      <th>embarked_S</th>\n","      <th>who_child</th>\n","      <th>who_woman</th>\n","      <th>alone_True</th>\n","      <th>adult_male_True</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>22.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>7.2500</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>38.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>71.2833</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>26.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.9250</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>35.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>53.1000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>35.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>8.0500</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   survived  pclass   age  sibsp  parch     fare  embarked_Q  embarked_S  \\\n","0         0       3  22.0      1      0   7.2500           0           1   \n","1         1       1  38.0      1      0  71.2833           0           0   \n","2         1       3  26.0      0      0   7.9250           0           1   \n","3         1       1  35.0      1      0  53.1000           0           1   \n","4         0       3  35.0      0      0   8.0500           0           1   \n","\n","   who_child  who_woman  alone_True  adult_male_True  \n","0          0          0           0                1  \n","1          0          1           0                0  \n","2          0          1           1                0  \n","3          0          1           0                0  \n","4          0          0           1                1  "]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["titanic = pd.get_dummies(titanic, columns=['embarked', 'who', 'alone', 'adult_male'])\n","titanic.drop(['embarked_C', 'who_man', 'alone_False', 'adult_male_False'], axis=1, inplace=True)\n","titanic.head()"]},{"cell_type":"markdown","metadata":{"id":"3xXXJR45o-U_"},"source":["Now that all of our variables are numeric we can continue with our logistic regression.\n","\n","Define a feature matrix (DataFrame) that includes all the variables except our target variable \"survived\". Call this DataFrame `X`. Check out the shape of `X` to make sure it makes sense. You can also visually inspect it by looking at the first few rows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aiL2mYvmo-U_"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QaOndJfmo-U_","outputId":"3046185d-97d3-467b-e084-d53540fd23da"},"outputs":[{"data":{"text/plain":["(891, 11)"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Don't run this cell to keep the outcome as your frame of reference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wmQ6v9pUo-U_"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-x')"]},{"cell_type":"markdown","metadata":{"id":"QMGZLpyBo-U_"},"source":["Create a target vector with \"survived\" and call it `y`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cu9AitIHo-U_"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqPI7kUOo-U_","outputId":"2476609d-db6e-4dba-f6f8-4176fea10fc5"},"outputs":[{"data":{"text/plain":["(891,)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Don't run this cell to keep the outcome as your frame of reference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNoCJ6dTo-U_"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-y')"]},{"cell_type":"markdown","metadata":{"id":"jjrolV3wo-U_"},"source":["We would like to evaluate the model on data it has not seen before, and so we will split the data into a training set and a testing set. Use a 30% split for test. You can use seed value 833 if you would like to get similar values as this notebook:"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"deletable":true,"editable":true,"id":"lOn-9KFco-U_"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7aAN5_B_o-U_"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-split')"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"2ffDmLtAo-U_"},"source":["With the data arranged, we can follow our recipe to predict the labels:\n","\n","First, instantiate a logistic regrssion model. You would first need to import `LogisticRegression`; it can be found under the `linear_model` module in `sklearn`. Call this model: `model`.\n","\n","While instantiating our model specify the `solver` as \"liblinear\". Solvers is the algorithm that will be used in the optimization problem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FZzFlBKao-U_"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OGn62pZwo-VC"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-instantiate')"]},{"cell_type":"markdown","metadata":{"id":"v31hHcwJo-VC"},"source":["Fit model to the training data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qayktc3yo-VC"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"npLiYyTOo-VC"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-fit')"]},{"cell_type":"markdown","metadata":{"id":"iLqoGyr6o-VC"},"source":["predict on new (test) data and store the results as `y_model`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8Hv_yvPo-VC"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tuFPhTEvo-VC"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-predict')"]},{"cell_type":"markdown","metadata":{"id":"uoDDUKgpo-VC"},"source":["Now that our predictions are ready we can merge them along with the ground truth, our `survived`, to the test features and visually inspect our model performance:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t0nWf-Pio-VC","outputId":"17129487-db8c-489e-b4b8-fbc641f2597e"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>pclass</th>\n","      <th>age</th>\n","      <th>sibsp</th>\n","      <th>parch</th>\n","      <th>fare</th>\n","      <th>embarked_Q</th>\n","      <th>embarked_S</th>\n","      <th>who_child</th>\n","      <th>who_woman</th>\n","      <th>alone_True</th>\n","      <th>adult_male_True</th>\n","      <th>survived</th>\n","      <th>predicted</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>243</td>\n","      <td>3</td>\n","      <td>22.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.1250</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>241</td>\n","      <td>3</td>\n","      <td>28.0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>15.5000</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>561</td>\n","      <td>3</td>\n","      <td>40.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.8958</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>108</td>\n","      <td>3</td>\n","      <td>38.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>7.8958</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>166</td>\n","      <td>1</td>\n","      <td>28.0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>55.0000</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   index  pclass   age  sibsp  parch     fare  embarked_Q  embarked_S  \\\n","0    243       3  22.0      0      0   7.1250           0           1   \n","1    241       3  28.0      1      0  15.5000           1           0   \n","2    561       3  40.0      0      0   7.8958           0           1   \n","3    108       3  38.0      0      0   7.8958           0           1   \n","4    166       1  28.0      0      1  55.0000           0           1   \n","\n","   who_child  who_woman  alone_True  adult_male_True  survived  predicted  \n","0          0          0           1                1         0          0  \n","1          0          1           0                0         1          1  \n","2          0          0           1                1         0          0  \n","3          0          0           1                1         0          0  \n","4          0          1           0                0         1          1  "]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["test = Xtest.join(ytest).reset_index()\n","test.join(pd.Series(y_model, name='predicted')).head()"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"7nQQupm7o-VC"},"source":["Finally, we can use the ``accuracy_score`` utility to see the fraction of predicted labels that match their true value:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0-juBV-Co-VC"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTgXe4Mro-VC","outputId":"43d77c1a-a6ef-4e01-ba0b-95790bf63d3e"},"outputs":[{"data":{"text/plain":["0.7873134328358209"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Don't run this cell to keep the outcome as your frame of reference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOs8ZrIxo-VC"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-acc')"]},{"cell_type":"markdown","metadata":{"deletable":true,"editable":true,"id":"kN2vMK6vo-VC"},"source":["Our basic model is giving us an accuracy of 79%. What accuracy can you reach by trying gaussian naive bayes? Repeat the steps for `GaussianNB`:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhEYIKu7o-VC"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NLysDvR2o-VD"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-GaussianNB')"]},{"cell_type":"markdown","metadata":{"id":"Jkh3T7ipo-VD"},"source":["And its accuracy on the test set?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tWFbWL8so-VD"},"outputs":[],"source":["# Your answer goes here\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iamS6qq-o-VD","outputId":"a14e6982-e68e-4b86-90d8-8e2c50e8fff1"},"outputs":[{"data":{"text/plain":["0.7657232704402517"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# Don't run this cell to keep the outcome as your frame of reference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5lox2EVo-VD"},"outputs":[],"source":["# SOLUTION: Uncomment and execute the line below to get help\n","#disp(hint + '11-02-acc')"]},{"cell_type":"markdown","source":["As we have finished the example, compare and contrast Logistic Regression and gaussian naive bayes model. Think about their similarities and differences in terms of:\n","\n","1.   The nature of the target variable\n","2.   Underlying Assumptions\n","3.   Interpretability\n","4.   Performance\n","5.   Limits\n"],"metadata":{"id":"wZ_fN9Xz1wzE"}},{"cell_type":"markdown","metadata":{"id":"XD1XH4n7o-VD"},"source":["There are several different ways to improve a model that we won't go into their details here but just mention them:\n","\n","* Model tuning or hyper-parameter tuning\n","* Feature engineering\n","* Trying out different models\n","* bringing new sources of data\n","\n","For other classification algorithms please visit the [Scikit learn documentation page](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"provenance":[{"file_id":"1nmOYOWsm2eNobJaX4svZbO9u9wXo2hCd","timestamp":1697012439358}]}},"nbformat":4,"nbformat_minor":0}